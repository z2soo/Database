{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FC layer: fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC layer: Fully Connected Layer, 혹은 Dense layer\n",
    "# 이전 layer의 모든 node가 다음 layer의 모든 node에 연결되어서 학습되는 layer 구조\n",
    "# 쉽게 말해서 지금까지의 학습은 FC layer 였음\n",
    "# 이전 layer의 출력값이 다음 layer의 입력값으로 사용되었기 때문!\n",
    "\n",
    "# 지금까지 우리가 작업한 신경망은 모두 FC layer를 이용하고 있었음!\n",
    "# FC layer의 특징: 입력 데이터(하나의 데이터, 한 사람에 대한 데이터)가 1차원으로 한정됨\n",
    "# 즉, 각각의 이미지가 1차원으로 표현이 되어야 한다.\n",
    "\n",
    "# 사실 이미지는 가로 * 세로 * 깊이(명암), 3차원의 데이터\n",
    "# 그렇지만 FC layer의 특징으로 인해 2차원 이미지(MNIST)를 1차원으로 변환해서 사용했음\n",
    "# MNIST는 흑백 이미지로 깊이가 불필요하였고, 가로*세로 정보는 784개의 col으로 변환되서 입력으로 넣은 것\n",
    "# MNIST 예제는 상당히 간단한 이미지 학습, 예측 예제!\n",
    "\n",
    "# 일반적으로 이미지 학습의 가장 큰 문제는(MNIST 제외 일반 이미지)\n",
    "# 이미지가 휘어있거나: 각도가 변경되어 있거나\n",
    "# 크기가 제각각이거나\n",
    "# 변형이 조금만 생겨도 학습이 힘들어진다.\n",
    "\n",
    "# 이미지가 항상 같은 각도, 크기, 형태로 주어지지 않기 때문에 학습이 어렵다.\n",
    "# 따라서 중요한 것은 그만큼 여러 종류의 이미지의 갯수, training data가 많아야 하고,\n",
    "# 많은 학습시간이 요구된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN: convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN: convolutional neural network\n",
    "# 이미지를 학습시키는 것에 있어서 현존하는 가장 효율적인 방법\n",
    "# CNN은 이미지 처리에 특화된 알고리즘 이다. \n",
    "\n",
    "# 많은 방법이 연구되어졌다.\n",
    "# 사람이 학습하는 방식(특징을 인식하는 것)을 모델링하였다. \n",
    "# 이미지의 픽셀 값을 그대로 학습하는 것이 아니라 \n",
    "# 이미지에 대한 특징을 도출해서 labeling 하여 신경망에 넣어서 학습하는 방식을 찾아냈다.\n",
    "\n",
    "# 하나의 이미지를 28*28 크기로 조정 및 다음과 같은 필터를 적용해서 여러번 학습\n",
    "# 포토샵 선명하게 하는 효과처럼 이미지를 sharp하게 만들어 학습\n",
    "# 흑백 이미지로 변환하여 학습 ... etc\n",
    "# 위와 같이 효과를 적용하였을 때 도드라지는 특징을 여러번! 학습시키는 것이 CNN의 방식이다.\n",
    "\n",
    "# 위의 과정을 학습할 때, 필터가 늘어나는 만큼 학습에 걸리는 시간이 늘어난다.\n",
    "# 이에 따라서 필터, 학습 시간이 늘어나면 이미지의 크기를 작게 조정한다.\n",
    "\n",
    "# 1장의 컬러 사진: width, height, color(depth: rgv 사용): 3차원\n",
    "# 여러 장의 사진이 사용되는 학습의 총 입력 데이터: 4차원\n",
    "\n",
    "# 평면화\n",
    "# 실제 이미지 1장은 3차원이고, 이를 flatten(평면화) 시켜서 1차원으로 표현해야 한다. \n",
    "# 그 과정에서 크기를 조절해야되기 때문에 공간에 대한 데이터를 유실할 우려가 있다.\n",
    "# 이런 데이터 유실 위험성 때문에 학습과 예측 과정에서 문제가 발생한다.\n",
    "\n",
    "# 공간 데이터 유실 위험성을 없애고, 이미지의 특성을 추출해서 학습을 용이하게 하기 위한 방식 = CNN\n",
    "# CNN은 공간 데이터 유실을 최소화 시키고, 특색을 도출하고, 한장으로 여러 특색을 도출!\n",
    "# deep learning 과정 이전에 CNN을 진행하고, 그 값을 FC layer(지금까지 우리가 한 과정)에 넣을 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel 혹은 filter\n",
    "# sharpen, blur, 흑백 등과 같은 효과 필터 또한 3차원: 3차원 이미지에 적용시킬 것이기 때문\n",
    "# 종류를 말하였지만, 필터는 임의로 만들어짐\n",
    "\n",
    "\n",
    "# 예시\n",
    "# 이미지 크기: 32*32*3\n",
    "# 필터의 크기: 5*5*3\n",
    "# 필터의 크기는 내가 설정할 수 있지만, depth는 이미지와 동일한 크기여야 한다.\n",
    "# 이때의 3은 channel이라고 표현: 색상이라고 칭하기는 의미가 너무 좁음\n",
    "# 하나의 이미지를 표현하는 방식 으로 생각! depth 외에 다른 방식이 이미지를 표현한다면 이 또한 channel\n",
    "# 컬러사진은 channel 3종류: green, red, blue, 흑백 사진은 gray one channel\n",
    "\n",
    "# 해당 개념 참고\n",
    "# http://taewan.kim/post/cnn/\n",
    "# 이미지 유실 가능성이 생김\n",
    "# \n",
    "# 이때, padding 처리 함: 원본과 동일한 사이즈로 feature map 출력\n",
    "# 생각해보면 이미지의 중요한 부분은 중앙, 따라서 padding한다고 하여 크게 왜곡되지 않음\n",
    "# padding을 할 것인지, stride 크기는 얼마일지 등 내가 결정\n",
    "# 이렇게 된 것이 activation map\n",
    "\n",
    "\n",
    "# feature map: 커널의 값과 필터 값을 곱해서 더해서 만듦: 계속 커지는 값\n",
    "# 따라서 relu 함수 적용: 원래 데이터 보다 적게 크기를 줄임(너무 크거나 작아지지 않도록)\n",
    "# 이렇게 특징을 뽑아내고 이 값을 입력값으로해서 또 convolution\n",
    "# 이렇게 나온 값들을 가지고 학습을 시킬 것\n",
    "\n",
    "\n",
    "# pooling layer\n",
    "# 이미지 자체의 크기가 너무 클 경우: pooling layer 사용 가능\n",
    "# 왜 stride를 크게 두고 사용하지 않고 pooling layer 사용?\n",
    "# stride가 커지면 feature를 잘 뽑아내지 못하기 때문: 듬성듬성 뽑아내기 때문\n",
    "\n",
    "# pooling layer 만드는 법\n",
    "# 마찬가지로 kernel, strid 존재\n",
    "# 곱하여 합 연산을 하였던, convolution 연산: feature map\n",
    "# pooling에서는 mapping\n",
    "\n",
    "# cnn에서는 대체로 max pool 사용, kernel을 activation map에 mapping했을 때, 가장 큰 값 하나를 뽑음\n",
    "# 마찬가지로 padding 사용 가능\n",
    "# 사용하지 않은 경우: size를 원본보다 작게 줄여서 학습에 용이하게 함\n",
    "# 사용하는 경우: 특징을 더욱 도드라지게 뽑아내기 위함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN code 구현 과정 1\n",
    "#### sample CNN을 가지고 함수, code에 대해서 알아보아요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 삽입\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image의 shape: (1, 3, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "# 이미지 데이터\n",
    "\n",
    "# 입력데이터의 형식 = width * height * depth : 3*3*1 이미지를 이용할 것 (depth가 1이다 = gray image)\n",
    "# 입력데이터: (이미지개수, wifth, height, depth) : (1,3,3,1) 4차원 배열로 표현\n",
    "# 총 9개의 데이터가 사용(1-9)\n",
    "# image shape = (1,3,3,1) 으로 뒤에서 부터 만들어가기\n",
    "\n",
    "image = np.array([[[[1],[2],[3]],\n",
    "                   [[4],[5],[6]],\n",
    "                   [[7],[8],[9]]]], dtype = np.float32)\n",
    "\n",
    "print(f'image의 shape: {image.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight shape: (2, 2, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "# filter 정의 \n",
    "\n",
    "# Activation map을 위한 filter 정의\n",
    "# (width, height, depth, filter 개수): depth는 이미지의 depth와 동일해야 함\n",
    "# filter shape = (2,2,1,3)으로 만들기\n",
    "\n",
    "weight = np.array([[[[1,2,3]],[[1,2,3]]], [[[1,2,3]],[[1,2,3]]]])\n",
    "\n",
    "print(f'weight shape: {weight.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d의 shape: (1, 2, 2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[12., 24., 36.],\n",
       "         [16., 32., 48.]],\n",
       "\n",
       "        [[24., 48., 72.],\n",
       "         [28., 56., 84.]]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convolution 실행: stride, padding 정의\n",
    "\n",
    "# stride = 1로 설정 : 가로, 세로를 1씩 움직인다.\n",
    "# tf.nn.con2d(적용 이미지, filter, strides=[4차원에 맞춰줘야 함], padding='')\n",
    "# tf에서 제공해주는 convolution 함수\n",
    "\n",
    "conv2d = tf.nn.conv2d(image, weight, \n",
    "                      strides =[1,1,1,1],padding = 'VALID')\n",
    "\n",
    "print(f'conv2d의 shape: {conv2d.shape}')\n",
    "\n",
    "# strides=[1,1,1,1]\n",
    "# 처음과 맨 끝 1은 dummp, 중간 두개가 가로, 세로에 대한 stride 설정 값\n",
    "# 가로, 세로 이동 크기 1,2 다르게 줄 수야 있지만 이상해짐, 같게 설정!\n",
    "\n",
    "# pading = 'SAME' : padding 처리하겟다.\n",
    "# pading = 'VALID' : padding 처리 안하겟다.\n",
    "\n",
    "sess = tf.Session()\n",
    "conv2d = sess.run(conv2d)\n",
    "conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool의 shape: (1, 2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "# pooling layer\n",
    "\n",
    "# pool = tf.nn.max_pool(해당 convoluted 값, ksize = [kernel size를 4차원에 맞춰주기], padding=\"\")\n",
    "pool = tf.nn.max_pool(conv2d, ksize = [1,2,2,1],\n",
    "                      strides = [1,1,1,1],\n",
    "                      padding='SAME')\n",
    "print(f'pool의 shape: {pool.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN code 구현 과정 2\n",
    "#### Convolutiona 결과 이미지가 원본 이미지에 비해 어떻게 다른지 확인해봐요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 삽입\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# data loading \n",
    "\n",
    "mnist = input_data.read_data_sets('./data/mnist', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img의 shape:(1, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# image 설정\n",
    "\n",
    "img = mnist.train.images[0].reshape(28,28)\n",
    "plt.imshow(img, cmap = 'Greys')\n",
    "\n",
    "# 해당 이미지르 convolutiona 처리를 해보아요\n",
    "# 입력 데이터: (이미지 개수, width, height, color depth) -> (1,3,3,1)\n",
    "# 2차원을 4차원으로 바꿔주기\n",
    "\n",
    "img = img.reshape(-1,28,28,1)\n",
    "print(f'img의 shape:{img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter 정의\n",
    "\n",
    "# 위에서는 임의로 잡았지만 이번에는 random \n",
    "W = tf.Variable(tf.random_normal([3,3,1,5]), name = 'filter1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d의 shape: (1, 14, 14, 5)\n",
      "conv2d_img의 shape: (5, 14, 14, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13e3b7c4710>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANAklEQVR4nO3df4xV9ZnH8fezwAgKKLrbpgVdaWJ0jXaXBsG2m64pNaHUCH/4h2bdsFuSjYm7taRJi9Gk2f82adO0yTZtiLU1C5E/1K6GQCuxNc0ai8UfURBbkbIyhQIbIjSoGUaf/eNek3EW1J7vuWdu5/t+JZP7Y+4zzzMTPjnnnnsO38hMJE1/fzbVA0jqhmGXKmHYpUoYdqkShl2qxMwum42MjOTs2bO7bClV5c0332RsbCzO9L1Owz579myWL1/eZUupKjt37jzr99yNlyph2KVKGHapEkVhj4iVEfHriNgXERvaGkpS+xqHPSJmAN8FPg9cCdwSEVe2NZikdpVs2ZcB+zJzf2aOAVuA1e2MJaltJWFfCByc8Hi0/9y7RMQ/R8SuiNh1+vTpgnaSSpSE/Uwf3P+/62Uzc2NmLs3MpbNmzSpoJ6lESdhHgYsnPF4EHCobR9KglIT9V8BlEbE4IkaAm4FH2hlLUtsany6bmeMR8S/AT4EZwL2Zuae1ySS1qujc+MzcBmxraRZJA+QZdFIlDLtUCcMuVcKwS5Uw7FIlDLtUCcMuVcKwS5Uw7FIlDLtUCcMuVcKwS5Uw7FIlDLtUCcMuVcKwS5Uw7FIlDLtUCcMuVcKwS5Uw7FIlDLtUiZJVXC+OiJ9HxN6I2BMRd7Q5mKR2lfy/8ePAVzLzmYiYBzwdETsy88WWZpPUosZb9sw8nJnP9O//AdjLGVZxlTQcWnnPHhGXAkuAnWf4nks2S0OgOOwRMRd4EPhyZp6c/H2XbJaGQ1HYI2IWvaBvzsyH2hlJ0iCUHI0P4AfA3sz8VnsjSRqEki37p4F/AD4bEc/1v1a1NJeklpWsz/7fQLQ4i6QB8gw6qRKGXapEyRl0nVuxYkXj2lWryg4n9I5HTo2SjyxnzJhR1Lu0/uGHH25cu3379qLemVlUP924ZZcqYdilShh2qRKGXaqEYZcqYdilShh2qRKGXaqEYZcqYdilShh2qRKGXaqEYZcqYdilSnR6ieucOXO4+uqrG9eXXKY6MjLSuBZgfHy8ce2uXbuKeh88eLBx7dtvv13U+/rrry+qX79+fePa888/v6j3li1bGtdOx8tj3bJLlTDsUiUMu1QJwy5Voo3ln2ZExLMRsbWNgSQNRhtb9jvoreAqaYiVrvW2CPgCcE8740galNIt+7eBrwJn/TB34pLNb7zxRmE7SU2VLOx4A3A0M59+r9dNXLJ5zpw5TdtJKlS6sOONEXEA2EJvgcdNrUwlqXWNw56Zd2bmosy8FLgZ+Flm3traZJJa5efsUiVauRAmMx8HHm/jZ0kaDLfsUiUMu1SJ6PK63Ygoanbbbbc1rj127FhJ6yJz584tqj969GhLk/zxDh06VFR/9913N6696aabinqvWbOmce2pU6eKek+VnTt3cvLkyTOuL+6WXaqEYZcqYdilShh2qRKGXaqEYZcqYdilShh2qRKGXaqEYZcqYdilShh2qRKGXaqEYZcq0emSzfPmzWP58uWN6/ft29fiNN05ceLEVI/Q2JIlS4rqX3nllca1x48fL+q9YMGCxrV/qpe4vhe37FIlDLtUCcMuVcKwS5UoXdjxgoh4ICJeioi9EfHJtgaT1K7So/HfAX6SmTdFxAhwbgszSRqAxmGPiPnAZ4B/BMjMMWCsnbEkta1kN/5jwDHghxHxbETcExHnTX7RxCWbT58+XdBOUomSsM8EPgF8LzOXAKeADZNfNHHJ5lmzZhW0k1SiJOyjwGhm7uw/foBe+CUNoZIlm38PHIyIy/tPrQBebGUqSa0rPRr/r8Dm/pH4/cA/lY8kaRCKwp6ZzwFLW5pF0gB5Bp1UCcMuVaLT69lrVXINP8Dq1asb15Zc0w3w2muvFdXPmzevqL7Edddd17h206ZN7Q0yJNyyS5Uw7FIlDLtUCcMuVcKwS5Uw7FIlDLtUCcMuVcKwS5Uw7FIlDLtUCcMuVcKwS5Uw7FIlDLtUiWquZ7/kkkuK6tetW9e4tvSa7ieeeKJx7Z49e4p6j4+PF9VfdNFFjWtvv/32ot7TcY31Em7ZpUoYdqkShl2qROmSzesjYk9E7I6I+yNidluDSWpX47BHxELgS8DSzLwKmAHc3NZgktpVuhs/E5gTETPprc1+qHwkSYNQstbb74BvAq8Ch4ETmfno5Ne5ZLM0HEp24xcAq4HFwEeB8yLi1smvc8lmaTiU7MZ/DvhtZh7LzNPAQ8Cn2hlLUttKwv4qcG1EnBsRQW/J5r3tjCWpbSXv2XcCDwDPAC/0f9bGluaS1LLSJZu/Dny9pVkkDZBn0EmVMOxSJaq5xHXr1q1F9YsXL25ce/DgwaLeR44caVybmUW9X3/99SmrHxsbK+r95JNPFtVPN27ZpUoYdqkShl2qhGGXKmHYpUoYdqkShl2qhGGXKmHYpUoYdqkShl2qhGGXKmHYpUoYdqkShl2qRDXXs19xxRVF9cePH29cu3///qLeU2nZsmVF9ddcc03j2tL/B2B0dLRx7cyZ0y8abtmlShh2qRKGXarE+4Y9Iu6NiKMRsXvCcxdGxI6IeLl/u2CwY0oq9UG27D8CVk56bgPwWGZeBjzWfyxpiL1v2DPzF8DkQ9Grgfv69+8D1rQ8l6SWNX3P/uHMPAzQv/3Q2V7oks3ScBj4ATqXbJaGQ9OwH4mIjwD0b4+2N5KkQWga9keAtf37a4GH2xlH0qB8kI/e7geeBC6PiNGIWAf8O3B9RLwMXN9/LGmIve8JwJl5y1m+taLlWSQNkGfQSZUw7FIlpt91fGcxMjJSVP/CCy+0NMmflgMHDhTVr1nT/Hyrxx9/vKj3dLxMtYRbdqkShl2qhGGXKmHYpUoYdqkShl2qhGGXKmHYpUoYdqkShl2qhGGXKmHYpUoYdqkShl2qhGGXKuEFv9NcRBTVv/XWW0X1Tz31VOPabdu2FfX2evZ3c8suVcKwS5Uw7FIlmi7Z/I2IeCkino+IH0fEBYMdU1Kppks27wCuysyPA78B7mx5Lkkta7Rkc2Y+mpnj/Ye/BBYNYDZJLWrjPfsXge0t/BxJA1QU9oi4CxgHNr/Ha1yfXRoCjc86iIi1wA3AiszMs70uMzcCGwHmz59/1tdJGqxGYY+IlcDXgL/LzNfbHUnSIDRdsvk/gHnAjoh4LiK+P+A5JRVqumTzDwYwi6QB8gw6qRKGXaqE1wBOc+/xQckHcs455xTVP/jgg41rvUS1XW7ZpUoYdqkShl2qhGGXKmHYpUoYdqkShl2qhGGXKmHYpUoYdqkShl2qhGGXKmHYpUoYdqkShl2qRJRe7/xHNYs4BvzPe7zkz4H/7Wgce9t7Ovb+y8z8izN9o9Owv5+I2JWZS+1tb3u3z914qRKGXarEsIV9o73tbe/BGKr37JIGZ9i27JIGxLBLlRiKsEfEyoj4dUTsi4gNHfa9OCJ+HhF7I2JPRNzRVe8JM8yIiGcjYmvHfS+IiAci4qX+7//JDnuv7/+9d0fE/RExe8D97o2IoxGxe8JzF0bEjoh4uX+7oMPe3+j/3Z+PiB9HxAWD6D3ZlIc9ImYA3wU+D1wJ3BIRV3bUfhz4Smb+FXAtcHuHvd9xB7C3454A3wF+kplXAH/d1QwRsRD4ErA0M68CZgA3D7jtj4CVk57bADyWmZcBj/Ufd9V7B3BVZn4c+A1w54B6v8uUhx1YBuzLzP2ZOQZsAVZ30TgzD2fmM/37f6D3D35hF70BImIR8AXgnq569vvOBz5Df4HOzBzLzNc6HGEmMCciZgLnAocG2SwzfwEcn/T0auC+/v37gDVd9c7MRzNzvP/wl8CiQfSebBjCvhA4OOHxKB0G7h0RcSmwBNjZYdtvA18F3u6wJ8DHgGPAD/tvIe6JiPO6aJyZvwO+CbwKHAZOZOajXfSe5MOZebg/02HgQ1MwA8AXge1dNBqGsMcZnuv088CImAs8CHw5M0921PMG4GhmPt1Fv0lmAp8AvpeZS4BTDG439l36741XA4uBjwLnRcStXfQeNhFxF723kpu76DcMYR8FLp7weBED3q2bKCJm0Qv65sx8qKu+wKeBGyPiAL23Lp+NiE0d9R4FRjPznb2YB+iFvwufA36bmccy8zTwEPCpjnpPdCQiPgLQvz3aZfOIWAvcAPx9dnSyyzCE/VfAZRGxOCJG6B2seaSLxhER9N637s3Mb3XR8x2ZeWdmLsrMS+n9zj/LzE62cJn5e+BgRFzef2oF8GIXventvl8bEef2//4rmJoDlI8Aa/v31wIPd9U4IlYCXwNuzMzXu+pLZk75F7CK3lHJV4C7Ouz7t/TeMjwPPNf/WjUFv/91wNaOe/4NsKv/u/8XsKDD3v8GvATsBv4TOGfA/e6nd3zgNL29mnXARfSOwr/cv72ww9776B2neuff3Pe7+Lt7uqxUiWHYjZfUAcMuVcKwS5Uw7FIlDLtUCcMuVcKwS5X4PyW3yGbuFLsmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convolution\n",
    "\n",
    "# 입력과 출력의 크기가 같게 나오게 하기 위해서는 padding=SAME 뿐 아니라 stride도 1이어야 함\n",
    "# \n",
    "conv2d = tf.nn.conv2d(img, \n",
    "                      W,\n",
    "                      strides = [1,2,2,1],\n",
    "                      padding = 'SAME')\n",
    "print(f'conv2d의 shape: {conv2d.shape}')\n",
    "      \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "conv2d = sess.run(conv2d)\n",
    "\n",
    "\n",
    "# 이미지를 표현하기 위해 축을 전환\n",
    "# (1,14,14,5) -> (5,14,14,1)\n",
    "conv2d_img = np.swapaxes(conv2d, 0, 3)\n",
    "print(f'conv2d_img의 shape: {conv2d_img.shape}')\n",
    "plt.imshow(conv2d_img[0].reshape(14,14), cmap=\"Greys\")\n",
    "# plt.imshow(conv2d_img[4].reshape(14,14), cmap=\"Greys\")\n",
    "# conv2d_img의 5번째 그림을 보인는데, 이를 위해 2차원으로 reshape\n",
    "# random normal 영향으로 실행시마다 이미지 그림이 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow MNIST with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 삽입\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로딩\n",
    "\n",
    "mnist = input_data.read_data_sets('./data/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 초기화\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "\n",
    "X = tf.placeholder(shape = [None, 784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape = [None, 10], dtype=tf.float32)\n",
    "drop_rate = tf.placeholder(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7, 7, 64)\n"
     ]
    }
   ],
   "source": [
    "# convolution layer 설정 과정\n",
    "\n",
    "\n",
    "# image\n",
    "x_img = tf.reshape(X, [-1,28,28,1])\n",
    "\n",
    "# filter\n",
    "# width, height, depth: image와 맞춰줌, filter 개수: 임의 설정\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32]), name ='Weight1')\n",
    "L1 = tf.nn.conv2d(x_img,\n",
    "                  W1,\n",
    "                  strides=[1,1,1,1],\n",
    "                  padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "# pooling은 convolution layer 끝나고 넣어도 되고 안넣어도 되고..\n",
    "L1 = tf.nn.max_pool(L1,\n",
    "                    ksize=[1,2,2,1],\n",
    "                    strides=[1,2,2,1],\n",
    "                    padding='SAME')\n",
    "\n",
    "\n",
    "# 한번하고 끝낼 수도있고, 더 할 수도 있음\n",
    "# 위의 filter 갯수가 convolution을 통해 depth로들어가기 때문에 depth에 32, 64: 임의설정\n",
    "W2 = tf.Variable(tf.random_normal([3,3,32,64]), name ='Weight2')\n",
    "L2 = tf.nn.conv2d(L1,\n",
    "                  W2,\n",
    "                  strides=[1,1,1,1],\n",
    "                  padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2,\n",
    "                    ksize=[1,2,2,1],\n",
    "                    strides=[1,2,2,1],\n",
    "                    padding='SAME')\n",
    "print(L2.shape)\n",
    "# 이제 이 값을 어디에 넣? :  FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC layer\n",
    "\n",
    "# FC에 위의 데이터를 넣어야 됨\n",
    "# FC는 값이 일차원 배열, 전체 2차원 배열 입력만 가능\n",
    "# 따라서 L2 reshape\n",
    "\n",
    "L2 = tf.reshape(L2, [-1, 7*7*64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi layer 설정\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", \n",
    "                     shape = [7*7*64,256],\n",
    "                     initializer = tf.contrib.layers.xavier_initializer())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[CPU_ENV]",
   "language": "python",
   "name": "cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
