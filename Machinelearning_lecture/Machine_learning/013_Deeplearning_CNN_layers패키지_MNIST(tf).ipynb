{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지금까지 배운내용을 기초로 tensorflow의 mnist example을 \n",
    "# cnn으로 구현해보아요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module 삽입\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "# 결측치, 이상치, 정규화, feature engineering\n",
    "# tf 제공 mnist 는 이미 전처리가 진행된 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 정의: tensorflow graph 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 초기화\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder\n",
    "\n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "# keep_rate = tf.placeholder(tf.float32)\n",
    "drop_rate = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image data 설정\n",
    "\n",
    "x_img = tf.reshape(X,[-1,28,28,1])\n",
    "\n",
    "# 입력 데이터 X를 4차원으로 변환\n",
    "# 해당 데이터는 흑백이기에 channel 1\n",
    "# 해당 데이터의 갯수는 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7, 7, 64)\n"
     ]
    }
   ],
   "source": [
    "# convolution 과정\n",
    "\n",
    "# 3*3*1 : 크기와 depth, 32 : channel 갯수\n",
    "'''F1 = tf.Variable(tf.random_normal([3,3,1,32]), name='filter1')\n",
    "L1 = tf.nn.conv2d(x_img,\n",
    "                  F1,\n",
    "                  strides = [1,1,1,1],\n",
    "                  padding = \"SAME\")\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1,\n",
    "                    ksize = [1,2,2,1],\n",
    "                    strides = [1,2,2,1],\n",
    "                    padding = \"SAME\")'''\n",
    "\n",
    "# layer 패키지 사용한 다른 방법\n",
    "# 필터를 따로 정의하지 않는 방법도 있음 : 하나의 식으로 표현\n",
    "# 데이터, 필터수, 필터사이즈, 패딩, 스트라이드, 이후의 작업\n",
    "# 단, pooling 작업은 따로 해줘야 되고 layers 패키지 사용하면서 다른 함수 사용\n",
    "L1 = tf.layers.conv2d(inputs = x_img,\n",
    "                      filters = 32,\n",
    "                      kernel_size = [3,3],\n",
    "                      padding =\"SAME\",\n",
    "                      strides = 1,\n",
    "                      activation = tf.nn.relu)\n",
    "L1 = tf.layers.max_pooling2d(inputs = L1,\n",
    "                             pool_size = [2,2],\n",
    "                             padding = \"SAME\",\n",
    "                             strides = 2)\n",
    "# 추가적으로 convolution 수행할 때 dropout 실행할 수 있음\n",
    "# 원래는 FC에서 실행하지면 시간을 더 줄이기 위해서 현재 과정에서 사용 가능\n",
    "# 이때는 rate 변수 사용\n",
    "L1 = tf.layers.dropout(inputs = L1, \n",
    "                       rate = drop_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "L2 = tf.layers.conv2d(inputs = L1,\n",
    "                      filters = 64,\n",
    "                      kernel_size = [3,3],\n",
    "                      padding =\"SAME\",\n",
    "                      strides = 1,\n",
    "                      activation = tf.nn.relu)\n",
    "L2 = tf.layers.max_pooling2d(inputs = L2,\n",
    "                             pool_size = [2,2],\n",
    "                             padding = \"SAME\",\n",
    "                             strides = 2)\n",
    "print(L2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense layer, Fully Connected layer 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 준비\n",
    "\n",
    "convoled_data = tf.reshape(L2, [-1,7*7*64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, b, layer\n",
    "\n",
    "'''W1 = tf.get_variable(\"weight1\", \n",
    "                     shape = [7*7*64, 256],\n",
    "                     initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "b1 = tf.Variable(tf.random_normal(shape=[256]),\n",
    "                 name = 'bias1')\n",
    "_layer1 = tf.nn.relu(tf.matmul(convoled_data, W1) + b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep_rate)'''\n",
    "\n",
    "# layer 패키지에서 제공해주는 함수 사용하면 더욱 간편하게 서술 가능\n",
    "# W, b에 대한 곱을 비롯한 모든 계산을 알아서 해줌\n",
    "layer1 = tf.layers.dense(inputs = convoled_data,\n",
    "                        units = 256,   #출력값\n",
    "                        activation = tf.nn.relu)\n",
    "\n",
    "# dropout 또 진행해도 좋음\n",
    "layer1 = tf.layers.dropout(inputs = layer1,\n",
    "                           rate = drop_rate)\n",
    "\n",
    "\n",
    "layer2 = tf.layers.dense(inputs = layer1,\n",
    "                        units = 128,   \n",
    "                        activation = tf.nn.relu)\n",
    "layer2 = tf.layers.dropout(inputs = layer2,\n",
    "                          rate = drop_rate)\n",
    "\n",
    "\n",
    "layer3 = tf.layers.dense(inputs = layer2,\n",
    "                        units = 512,  \n",
    "                        activation = tf.nn.relu)\n",
    "layer3 = tf.layers.dropout(inputs = layer2,\n",
    "                          rate = drop_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H\n",
    "\n",
    "'''logit = tf.matmul(layer2, W3) + b3\n",
    "H = tf.nn.softmax(logit)'''\n",
    "# softmax 내가 적용하지 않아도 내부에서 모두 적용\n",
    "\n",
    "H = tf.layers.dense(inputs = layer3, units=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:691: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cost\n",
    "\n",
    "'''cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,\n",
    "                                                                 labels = Y))'''\n",
    "cost = tf.losses.softmax_cross_entropy(Y, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "train = tf.train.AdadeltaOptimizer(learning_rate = 0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결국 우리의 mnist 에제는 multinomial 예제\n",
    "# 이미지 1개에 대한 예측값, H 도출값은 [0.5, 0.3, 0.2, 0.001, 0.99, 0.44 ...]\n",
    "\n",
    "# xavier를 사용한다고 하더라도 결론적으로 w, b의 값은 초기에 랜덤으로 잡힘\n",
    "# 즉, 학습을 새로 시킬때마다 학습의 결과가 매번 다르게 나올 수 밖에 없음\n",
    "\n",
    "# ensenble은 모델을 여러개 만들고 (10개의 모델)\n",
    "# 이미지 1개에 대한 각 모델의 예측값을 구한다.\n",
    "# 모델1: [0.5, 0.3, 0.2, 0.001, 0.99, 0.44 ...]\n",
    "# 모델2: [0.4, 0.3, 0.11, 0.05, 0.90, 0.64 ...] ...\n",
    "\n",
    "# 이 모든 모델의 값을 모두 더하여 prediction을 한다.\n",
    "# sun = [0.9, 0.6, ... ] \n",
    "\n",
    "# 프로그램을 사용하는 것이 아닌 logic으로 해결해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[CPU_ENV]",
   "language": "python",
   "name": "cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
