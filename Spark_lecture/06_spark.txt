머신러닝 :
기계 스스로 데이터를 이용해 문제를 해결할 수 있는 알고리즘을 만드는 것
입력 데이터로부터 원하는 결과값(출력값)을 얻어내는 것이 목적
예] 회귀 알고리즘 LogisticRegression -> fit() - >LogisticRegressionModel

Parametric 알고리즘 : 
고정된 개수의 파라미터, 계수를 사용하는 것
입력과 출력 사이의 관계를 특성 값에 관한 수학적 함수 또는 수식으로 가정
수식의 결과가 실제 결과값에 가깝도록 계수를 조정하는 방법을 사용
예] 선형회귀(Linear Regression) , 로지스틱(Logistic Regression)

Nonparametric 알고리즘  :
입력과 출력 사이의 가설을 세우지 않고 머신러닝의 수행 결과를 그대로 사용하는 방식
예] SVM(Support Vector Machine), 나이브 베이즈(Nativ Bayes)  

머신러닝 활용 : 
유사한 것들끼리 묶거나 분류하기
인터넷 쇼핑몰 등에서 사용자의 과거 행동 패턴을 바탕으로 상품 추천하기
시스템이나 사용자의 비정상적 상태를 검출하거나 예측하기
이벤트나 상품의 보이지 않는 관계 찾아내기
텍스트 정보로부터 긍정적/부정적 표현 여부를 판정하는 등 텍스트의 의미 정보 추정하기

지도 학습(Supervised Learning) : 
훈련 데이터에 레이블(정답)에 관한 정보가 포함되며 알고리즘은 입력과 출력에 대한 가설과 정답 정보를 이용해 오차를 계산하고 이를 통해 입력과 출력 사이의 관계를 유추하게 됩니다.

비지도 학습 (Unsupervised Learning) :
특성과 레이블 간의 인과 관계를 모르거나 특별히 지정하지 않고 컴퓨터의 처리에 맡기는 것
예] Clustering


Spark MLlib API 주요 기능 : 
- 머신러닝 알고리즘 : classification, Regression, Clustering, collaborative filtering 등
- 특성 추출, 변환, 선택
- 파이프라인(Pipeline) : 여러 종류의 머신러닝 알고리즘을 순차적으로 수행할 수 있도록 제공
- 저장 : 알고리즘, 모델, 파이프라인에 대한 저장 및 불러오기 기능 제공
- 유틸리티 : 선형대수, 통계, 데이터 처리 등의 유용한 함수를 제공


벡터 :  1차원 데이터를 다루는 데이터 타입
LocalVector - 기본적인 벡터 타입으로 밀집 벡터인 DenseVector 클래스와 희소 벡터인 SparseVector 클래스 
LabeledPoint - LocalVector에 레이블을 부여한 데이터 타입. 예) 지도학습 알고리즘을 이용할 때 독립변수와 종속변수를 함께 보존할 때 이용


Transformer - 데이터를 변환처리를 시행하는 컴포넌트
                   모델의 입력변수로 사용할 특징을 적절히 구성(특징 수를 줄이거나, 특징을 추가, 데이터 조작 등)
                  모델의 입력변수는 double 타입이나 vector로 구성되어야 합니다.

Estimator - 데이터프레임에 알고리즘을 적용해 새로운 트랜스포머(머신러닝 모델)를 생성하는 역할
모든 Estimator 클래스는 Transformer를 생성하는 fit() 메서드를 가짐

Pipeline -  여러 알고리즘을 순차적으로 실행할 수 있는 워크플로우를 생성하는 평가자


from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Test3").config("spark.executor.instances", "1").config("spark.executor.cores", "1").config("spark.executor.memory", "2g").config("spark.driver.memory", "2g").getOrCreate()
df = spark.read.json("/home/tutor/data/simple-ml")
df.orderBy("value2").show()

from pyspark.ml.feature import RFormula

supervised = RFormula(formula="lab ~ . +color:value1 + color:value2")
#선언된 formaula에 따라 데이터를 변환할 객체(모델) 생성
fittedRF = supervised.fit(df)   
 # 데이터 변환 객체를 이용하여 데이터 변환
preparedRF = fittedRF.transform(df) 
preparedRF.show(20, False)

#학습, 테스트 데이터 분리 ( 7:3)
train, test = preparedRF.randomSplit([0.7, 0.3])

#로지스틱 회귀 알고리즘 객체 생성 
from pyspark.ml.classification import LogisticRegression
lr = LogisticRegression(labelCol="label",featuresCol="features")

#로지스틱 회귀 알고리즘의 파라미터들 설명 확인
print lr.explainParams()

#로지스틱 회귀 알고리즘 적용(학습)하여 모델 생성
fittedLR = lr.fit(train)

#모델로부터 학습 데이터의 예측값과 실제값 출력 

# 정확도 평가 후 하이퍼 파라미터 조정해서 학습 후 평가 -> 하이퍼 파라미터 조정해서 학습 후 평가를 반복 수행해야 하므로 ...
Pipeline으로 캡슐화 (적용할 모델과  모델의 하이퍼파라미터와 평가자를 조합한 ParmMap을 설정) 하여 수행하는 것을 권장함 (Spark에서 ML 수행 패턴)

#pipeline으로 수행하기 위해 데이터를 임의 분할
train, test = df.randomSplit([0.7, 0.3])

# 특성 변환을 위한 RFormaul객체 생성
rForm = RFormula()
lr = LogisticRegression().setLabelCol("label").setFeaturesCol("features")

#pipeline객체 생성 (수행 단계 설정 - RFormaula와 LogisticRegression객체)
from pyspark.ml import Pipeline
stages = [rForm, lr]
pipeline = Pipeline().setStages(stages)

#RFormaula와 하이퍼파라미터를 12가지조합으로 학습시킬 ParamMap객체 생성
from pyspark.ml.tuning import ParamGridBuilder
params = ParamGridBuilder().addGrid(rForm.formula, [
    "lab ~ . + color:value1",
    "lab ~ . + color:value1 + color:value2"]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).addGrid(lr.regParam, [0.1, 2.0]).build()

#이진분류 평가기 객체 생성
from pyspark.ml.evaluation import BinaryClassificationEvaluator
evaluator = BinaryClassificationEvaluator().setMetricName("areaUnderROC").setRawPredictionCol("prediction").setLabelCol("label")

#ParamMap, 추정자Pipeline, 평가기 , 학습 데이터 를 설정
from pyspark.ml.tuning import TrainValidationSplit
tvs = TrainValidationSplit().setTrainRatio(0.75).setEstimatorParamMaps(params).setEstimator(pipeline).setEvaluator(evaluator)

#학습 시킴(->모델 생성)
tvsFitted = tvs.fit(train)

#평가하기  (테스트데이터에 모델 적용)
evaluator.evaluate(tvsFitted.transform(test)) 



####StandardScaler(평균 0, 분산 1이 되도록 데이터 변환) 실습 #####
scaleDF = spark.read.parquet("/home/tutor/data/simple-ml-scaling/")
scaleDF.show()

from pyspark.ml.feature import StandardScaler
sScaler = StandardScaler().setInputCol("features")
sScaler.fit(scaleDF).transform(scaleDF).show()

########버켓팅으로 데이터 변환(경계값을 이용하여 버켓 생성) ###
contDF = spark.range(20).selectExpr("cast(id as double)")
contDF.show()

from pyspark.ml.feature import Bucketizer

bucketBorders = [-1.0 , 0.5, 5.0, 10.0, 15.0, 20.0 ]
bucketer = Bucketizer().setSplits(bucketBorders).setInputCol("id")
bucketer.transform(contDF).show()

scaleDF = spark.read.parquet("/home/tutor/data/simple-ml-scaling/")
scaleDF.show()

from pyspark.ml.feature import StandardScaler
sScaler = StandardScaler().setInputCol("features")
sScaler.fit(scaleDF).transform(scaleDF).show()

from pyspark.ml.feature import MinMaxScaler
minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol("features")
minMax.fit(scaleDF).transform(scaleDF).show()

from pyspark.ml.feature import MaxAbsScaler
maScaler = MaxAbsScaler(). setInputCol("features")
maScaler.fit(scaleDF).transform(scaleDF).show()

from pyspark.ml.feature import ElementwiseProduct
from pyspark.ml.linalg import Vectors
scaleVec = Vectors.dense(10.0, 15.0, 20.0)
scalingUp = ElementwiseProduct(). setScalingVec(scaleVec).setInputCol("features")
scalingUp.transform(scaleDF).show()


from pyspark.ml.feature import Normalizer
manhattenDistance = Normalizer().setP(1).setInputCol("features")
manhattenDistance.transform(scaleDF).show()
uclideDistance = Normalizer().setP(2).setInputCol("features")
uclideDistance.transform(scaleDF).show()

###########범주형 데이터 인덱싱화(변환) ###############
from pyspark.ml.feature import StringIndexer

simpleDF = spark.read.json("/home/tutor/data/simple-ml")

valIndexer = StringIndexer().setInputCol("value1").setOutputCol("valueInd")
valIndexer.fit(simpleDF).transform(simpleDF).show()

##텍스트 처리 (불용어 제거, ngram)#####################

sales = spark.read.format("csv") .option("header", "true")\
  .option("inferSchema", "true").load("/home/tutor/data/retail-data/by-day/*.csv").coalesce(5).where("Description IS NOT NULL")

from pyspark.ml.feature import Tokenizer
tkn = Tokenizer().setInputCol("Description").setOutputCol("DescOut")
tokenized = tkn.transform(sales.select("Description"))
tokenized.show(20, False)

from pyspark.ml.feature import StopWordsRemover
englishStopWords = StopWordsRemover.loadDefaultStopWords("english")
stops = StopWordsRemover().setStopWords(englishStopWords)  .setInputCol("DescOut")
stops.transform(tokenized).show()

from pyspark.ml.feature import RegexTokenizer
rt = RegexTokenizer()
  .setInputCol("Description")
  .setOutputCol("DescOut")
  .setPattern(" ")  
  .setToLowercase(True)
rt.transform(sales.select("Description")).show(20, False)
sales.show()

from pyspark.ml.feature import NGram
unigram = NGram().setInputCol("DescOut").setN(1)
bigram = NGram().setInputCol("DescOut").setN(2)
unigram.transform(tokenized.select("DescOut")).show(10, False)
bigram.transform(tokenized.select("DescOut")).show(10, False)


from pyspark.ml.feature import CountVectorizer
cv = CountVectorizer().setInputCol("DescOut").setOutputCol("countVec").setVocabSize(500).setMinTF(1).setMinDF(2)
fittedCV = cv.fit(tokenized)
fittedCV.transform(tokenized).show(10, False)


tfIdfIn = tokenized.where("array_contains(DescOut, 'red')").select("DescOut").limit(10)
tfIdfIn.show(10, False)

 
from pyspark.ml.feature import HashingTF, IDF
tf = HashingTF().setInputCol("DescOut").setOutputCol("TFOut").setNumFeatures(10000)
idf = IDF().setInputCol("TFOut").setOutputCol("IDFOut").setMinDocFreq(2)
 
idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False)



from pyspark.ml.feature import Word2Vec
 
documentDF = spark.createDataFrame([
    ("Hi I heard about Spark".split(" "), ),
    ("I wish Java could use case classes".split(" "), ),
    ("Logistic regression models are neat".split(" "), )
], ["text"])
 
word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol="text",
  outputCol="result")
model = word2Vec.fit(documentDF)
result = model.transform(documentDF)
for row in result.collect():
    text, vector = row
    print("Text: [%s] => \nVector: %s\n" % (", ".join(text), str(vector)))




