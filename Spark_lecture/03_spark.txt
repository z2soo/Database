#구조체 - DataFrame내부의 DataFrame
from pyspark.sql.functions import struct
complexDF = df.select(struct("Description", "InvoiceNo").alias("complex"))
complexDF.createOrReplaceTempView("complexDF")
complexDF
complexDF.show()

# 배열  생성 -split() 
from pyspark.sql.functions import split
df.select(split(col("Description"), " ")).show(2)

df.select(split(col("Description"), " ").alias("array_col")) 
  .selectExpr("array_col[0]").show(2)

from pyspark.sql.functions import size
df.select(size(split(col("Description"), " "))).show(2) # 5와 3 출력

from pyspark.sql.functions import array_contains
df.select(array_contains(split(col("Description"), " "), "WHITE")).show(2)


from pyspark.sql.functions import split, explode

df.withColumn("splitted", split(col("Description"), " ")) 
  .withColumn("exploded", explode(col("splitted"))) 
  .select("Description", "InvoiceNo", "exploded").show(2)


#맵 생성
from pyspark.sql.functions import create_map
df.select(create_map(col("Description"), col("InvoiceNo")).alias("complex_map")).show(2)


df.select(create_map(col("Description"), col("InvoiceNo")).alias("complex_map")).selectExpr("complex_map['WHITE METAL LANTERN']").show(2)

#Map의 키와 값을 다시 컬럼으로 변환
df.select(create_map(col("Description"), col("InvoiceNo")).alias("complex_map")).selectExpr("explode(complex_map)").show(2)

#JSON 컬럼 생성
jsonDF = spark.range(1).selectExpr("""
  '{"myJSONKey" : {"myJSONValue" : [1, 2, 3]}}' as jsonString""")
jsonDF.show()
 
#get_json_object, json_tuple함수로 JSON객체 조회
from pyspark.sql.functions import get_json_object, json_tuple

jsonDF.select(
    get_json_object(col("jsonString"), "$.myJSONKey.myJSONValue[1]").alias("column"),
    json_tuple(col("jsonString"), "myJSONKey")).show(2)

#JSON객체를 문자열로 변환 to_json
#JSON형식의 문자열을 다시 JSON객체로 변환 from_json
from pyspark.sql.functions import to_json
df.selectExpr("(InvoiceNo, Description) as myStruct").select(col("myStruct")).show(2, False)
df.selectExpr("(InvoiceNo, Description) as myStruct").select(to_json(col("myStruct"))).show(2, False)

 

from pyspark.sql.functions import from_json
from pyspark.sql.types import *
parseSchema = StructType((
  StructField("InvoiceNo",StringType(),True),
  StructField("Description",StringType(),True)))
df.selectExpr("(InvoiceNo, Description) as myStruct")
  .select(to_json(col("myStruct")).alias("newJSON"))
  .select(from_json(col("newJSON"), parseSchema), col("newJSON")).show(2, False)









df = spark.read.format("csv") 
  .option("header", "true") 
  .option("inferSchema", "true") 
  .load("/home/tutor/data/2010*.csv") 
  .coalesce(5)
df.cache()
df.createOrReplaceTempView("dfTable") 

from pyspark.sql.functions import count
df.select(count("StockCode")).show() # 541909

from pyspark.sql.functions import countDistinct
df.select(countDistinct("StockCode")).show() # 4070

from pyspark.sql.functions import approx_count_distinct
df.select(approx_count_distinct("StockCode", 0.1)).show() # 3364

from pyspark.sql.functions import first, last
df.select(first("StockCode"), last("StockCode")).show()

from pyspark.sql.functions import min, max
df.select(min("Quantity"), max("Quantity")).show()
 

from pyspark.sql.functions import sum
df.select(sum("Quantity")).show() # 5176450

from pyspark.sql.functions import sumDistinct
df.select(sumDistinct("Quantity")).show() # 29310
 
from pyspark.sql.functions import sum, count, avg, expr
df.select(
    count("Quantity").alias("total_transactions"),
    sum("Quantity").alias("total_purchases"),
    avg("Quantity").alias("avg_purchases"),
    expr("mean(Quantity)").alias("mean_purchases"))
  .selectExpr("total_purchases/total_transactions", "avg_purchases",    "mean_purchases").show()

from pyspark.sql.functions import var_pop, stddev_pop
from pyspark.sql.functions import var_samp, stddev_samp
df.select(var_pop("Quantity"), var_samp("Quantity"),
  stddev_pop("Quantity"), stddev_samp("Quantity")).show()


from pyspark.sql.functions import skewness, kurtosis
df.select(skewness("Quantity"), kurtosis("Quantity")).show()

from pyspark.sql.functions import corr, covar_pop, covar_samp
df.select(corr("InvoiceNo", "Quantity"), covar_samp("InvoiceNo", "Quantity"),
    covar_pop("InvoiceNo", "Quantity")).show()


from pyspark.sql.functions import collect_set, collect_list
df.agg(collect_set("Country"), collect_list("Country")).show()

#그룹화 처리
from pyspark.sql.functions import count

df.groupBy("InvoiceNo").agg(
    count("Quantity").alias("quan"),
    expr("count(Quantity)")).show()

df.groupBy("InvoiceNo").agg(expr("avg(Quantity)"),expr("stddev_pop(Quantity)"))\
  .show()


from pyspark.sql.functions import col, to_date
dfWithDate = df.withColumn("date", to_date(col("InvoiceDate"), "MM/d/yyyy H:mm"))
dfWithDate.createOrReplaceTempView("dfWithDate")


 

from pyspark.sql.window import Window
from pyspark.sql.functions import desc
windowSpec = Window\
  .partitionBy("CustomerId", "date")\
  .orderBy(desc("Quantity"))\
  .rowsBetween(Window.unboundedPreceding, Window.currentRow)


 

from pyspark.sql.functions import max
maxPurchaseQuantity = max(col("Quantity")).over(windowSpec)


 

from pyspark.sql.functions import dense_rank, rank
purchaseDenseRank = dense_rank().over(windowSpec)
purchaseRank = rank().over(windowSpec)


 

from pyspark.sql.functions import col

dfWithDate.where("CustomerId IS NOT NULL").orderBy("CustomerId")  .select(col("CustomerId"), col("date"), col("Quantity"),
    purchaseRank.alias("quantityRank"),
    purchaseDenseRank.alias("quantityDenseRank"),
    maxPurchaseQuantity.alias("maxPurchaseQuantity")).show()


 dfNoNull = dfWithDate.drop()
dfNoNull.createOrReplaceTempView("dfNoNull")


 
#rollup 
rolledUpDF = dfNoNull.rollup("Date", "Country").agg(sum("Quantity"))  .select(col("Date"), col("Country"), sum(col("Quantity").alias("total_quantity") .orderBy("Date")
rolledUpDF.show()


 
#cube
from pyspark.sql.functions import sum

dfNoNull.cube("Date", "Country").agg(sum(col("Quantity"))) 
  .select("Date", "Country", "sum(Quantity)").orderBy("Date").show()

#grouping_id
from pyspark.sql.functions import grouping_id ,sum ,expr

dfNoNull.cube("Date", "Country").agg(grouping_id(), sum("Quantity")).orderBy(col("grouping_id()").desc).show()
 
#pivot
pivoted = dfWithDate.groupBy("date").pivot("Country").sum() 



######################Review정리 #########################

#SparkSQL
#SQL을 원하는 이유
#익숙한 언어인 SQL로 데이터를 분석하고 싶어서
#맵리듀스보다 빠르고 편함
#정형화된 데이터의 경우 높은 최적화를 구현 가능

#Spark DataFrame을 이용한 불리언 데이터 처리#
#불리언 구문 - and, or, true, false

#pyspark의  jupyter notebook에서 SparkSession객체 생성하기
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Test").config("spark.executor.instances", "1").config("spark.executor.cores", "1").config("spark.executor.memory", "2g").config("spark.driver.memory", "2g").getOrCreate()

df = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/home/tutor/data/2010-12-01.csv")
df.printSchema()
df.createOrReplaceTempView("dfTable")

from pyspark.sql.functions import col
df.where(col("InvoiceNo") != 536365)
  .select("InvoiceNo", "Description")
  .show(5, False)  #결과 집합 Truncating 하지 못하도록 False 인자 사용


#불리언 표현식을 사용하는 경우 모든 표현식을 and()로 묶어 차례대로 필터 적용해야 함
from pyspark.sql.functions import instr
priceFilter = col("UnitPrice") > 600
descripFilter = instr(df.Description, "POSTAGE") >= 1
df.where(df.StockCode.isin("DOT")).where(priceFilter | descripFilter).show()



#Spark DataFrame을 이용한 수치데이터 처리#
from pyspark.sql.functions import  pow #지수만큼 컬럼의 값을 제곱

df.selectExpr(
  "CustomerId",
  "(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity").show(2)


from pyspark.sql.functions import lit, round, bround  #반올림, 내림
df.select(round(lit("2.5")), bround(lit("2.5"))).show(2)


from pyspark.sql.functions import corr  #피어슨 상관계수 계산 
df.stat.corr("Quantity", "UnitPrice")
df.select(corr("Quantity", "UnitPrice")).show()
 
df.describe().show() #컬럼에 대한 집계, 평균, 표준편차, 최솟값, 최대값 계산(콘솔 확인용)

from pyspark.sql.functions import count, mean, stddev_pop, min, max

colName = "UnitPrice"
quantileProbs = [0.5]
relError = 0.05
df.stat.approxQuantile("UnitPrice", quantileProbs, relError) # 백분위수를 정확하게 계산

df.stat.crosstab("StockCode", "Quantity").show()   #교차 테이블(표) 생성
df.stat.freqItems(["StockCode", "Quantity"]).show() 

from pyspark.sql.functions import monotonically_increasing_id
df.select(monotonically_increasing_id()).show(2)  #모든 로우에 ID값 추가

from pyspark.sql.functions import  rand, randn  #난수 생성


#Spark DataFrame을 이용한 문자열데이터 처리#
from pyspark.sql.functions import initcap
from pyspark.sql.functions import lower, upper
from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim
from pyspark.sql.functions import regexp_replace
from pyspark.sql.functions import translate
from pyspark.sql.functions import regexp_extract
from pyspark.sql.functions import instr


#Spark DataFrame을 이용한 날짜데이터 처리#
#스파크는 자바의 날짜와 타임스탬프를 사용해서 표준체계를 따름
from pyspark.sql.functions import current_date, current_timestamp   
#오늘 날짜와 현재 타임스태프값 반환
dateDF = spark.range(10) 
  .withColumn("today", current_date()) 
  .withColumn("now", current_timestamp())
dateDF.createOrReplaceTempView("dateTable")

#날짜 컬럼에 수치데이터 산술연산
from pyspark.sql.functions import date_add, date_sub  
dateDF.select(date_sub(col("today"), 5), date_add(col("today"), 5)).show(1)

###날짜 컬럼에 날짜데이터 산술연산
from pyspark.sql.functions import datediff, months_between, to_date
dateDF.withColumn("week_ago", date_sub(col("today"), 7)) 
  .select(datediff(col("week_ago"), col("today"))).show(1)

dateDF.select(
    to_date(lit("2016-01-01")).alias("start"),
    to_date(lit("2017-05-22")).alias("end")) 
  .select(months_between(col("start"), col("end"))).show(1)

#날짜 형식의 문자열 데이터를 날짜 데이터로 변환
from pyspark.sql.functions import to_date, lit
spark.range(5).withColumn("date", lit("2017-01-01"))
  .select(to_date(col("date"))).show(1)


from pyspark.sql.functions import to_date
dateFormat = "yyyy-dd-MM"
cleanDateDF = spark.range(1).select(
    to_date(lit("2017-12-11"), dateFormat).alias("date"),
    to_date(lit("2017-20-12"), dateFormat).alias("date2"))
cleanDateDF.createOrReplaceTempView("dateTable2")

#날짜 형식의 문자열 데이터를 타임스태프 데이터로 변환
from pyspark.sql.functions import to_timestamp
cleanDateDF.select(to_timestamp(col("date"), dateFormat)).show()


#Spark DataFrame을 이용한 null데이터 처리#

#여러 컬럼값중 null이 아닌 첫번째 값을 반환
from pyspark.sql.functions import coalesce
df.select(coalesce(col("Description"), col("CustomerId"))).show()

#null컬럼 row 삭제
df.na.drop("all", subset=["StockCode", "InvoiceNo"])  --any 옵션값 사용 가능

#null컬럼값을 다른 값으로 채움
df.na.fill("all", subset=["StockCode", "InvoiceNo"])

fill_cols_vals = {"StockCode": 5, "Description" : "No Value"}
df.na.fill(fill_cols_vals)

df.na.replace([""], ["UNKNOWN"], "Description")

########################################################
#스파크에서 사용자 정의함수 사용 가능하며, 반드시 등록후 사용해야 함
#버전 불일치로 발생되는 사용자 정의 함수 해결 방법 : 가상환경의 python버전을 spark의 python버전으로 맞춥니다.
conda create --name spark_tutorial python=3.6.8 pyspark jupyter
conda activate spark_tutorial
jupyter-notebook --ip=0.0.0.0 --no-browser --port=8889

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Test").config("spark.executor.instances", "1").config("spark.executor.cores", "1").config("spark.executor.memory", "2g").config("spark.driver.memory", "2g").getOrCreate()

#스파크에서 사용자 정의함수 사용 가능하며, 반드시 등록후 사용해야 함
udfExampleDF = spark.range(5).toDF("num")
def power3(double_value):
  return double_value ** 3     #함수 정의
power3(2.0)   #함수 호출

from pyspark.sql.functions import udf
power3udf = udf(power3)   #스파크에서 사용자 정의함수 등록

# 등록된 사용자 정의 함수 사용
from pyspark.sql.functions import col
 
udfExampleDF.select(power3udf(col("num"))).show(2)  
 
